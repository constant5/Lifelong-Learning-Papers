# Continual Lifelong Learning Papers

Contributed by Constant Marks. 

## [Content](#content)

<table>
<tr><td colspan="2"><a href="#survey-papers">1. Survey</a></td></tr> 
<tr><td colspan="2"><a href="#models">2. Models</a></td></tr>
<tr>
    <td>&emsp;<a href="#Lifelong-Learning-Models">2.1 Lifelong Learning Models</a></td>
    <td>&emsp;<a href="#Regularization-Approach">2.3 Regularization Approach</a></td>
</tr>
<tr>
    <td>&emsp;<a href="#Regularization-Approach-+">2.3 Regularization Approach +</a></td>
    <td>&ensp;<a href="#Dynamic-Architecture">2.4 Dynamic Architecture</a></td>
</tr>
<tr>
    <td>&emsp;<a href="#Complementary-System-and-Replay">2.5 Complementary System and Replay</a></td>
    <td>&ensp;<a href="#placeholder"></a></td>
</tr>

<tr><td colspan="2"><a href="#datasets">3. Datasets</a></td></tr> 
<tr>
    <td>&emsp;<a href="#MNIST">3.1 MNIST</a></td>
    <td>&ensp;<a href="#CUB-200">3.2 CUB-200</a></td>
</tr>
<tr>
    <td>&emsp;<a href="#AudioSet">3.3 AudioSet</a></td>
    <td>&ensp;<a href="#Core50">3.4 Core50</a></td>
</tr>
<tr>
    <td>&emsp;<a href="#AudioSet">3.5 iCIFAR-100</a></td>
    <td>&ensp;<a href="#"></a></td>
</tr> 

<tr><td colspan="2"><a href="#applications">4. Applications</a></td></tr> 
<tr>
    <td>&emsp;<a href="#placeholder2">4.1 placeholder2</a></td>
    <td>&ensp;<a href="#placeholder3">4.2 placeholder3</a></td>
</tr> 
</table>

## [Survey papers](#content)
1. **Continual Lifelong Learning with Neural Networks: A Review.** arXiv 2018. 

&emsp;&emsp;&emsp;*Parisi, G. et al.* [paper](https://arxiv.org/pdf/1802.07569.pdf)

1. **Lifelong Machine Learning Systems: Beyond Learning Algorithms** AAAI 2013 

&emsp;&emsp;&emsp;*Silver, D., Yang, Q., Li, L.* [paper](https://www.researchgate.net/profile/Daniel_Silver/post/Lifelong_Machine_Learning-how_important_do_you_feel_it_will_be_to_AI/attachment/59d61d9d79197b80779788c9/AS:271835600490496@1441822065030/download/Silver_Yang_AAAI_LML_Symposium.pdf)

1. **Learning in nonstationary environments:A survey** IEEE Computational Intelligence Magazine 2015. 

&emsp;&emsp;&emsp;*Ditzler, G. et al.* [paper](https://www.academia.edu/download/45784580/2015_-_Learning_in_Nonstationary_Environments_-_A_Survey_-_IEEE_CIM.pdf)

1. **The stability-plasticity dilemma: Investigating the continuum from catastrophic forgetting to age-limited learning effects** Frontiers in Psychology 2013. 

&emsp;&emsp;&emsp;*Mermillod, M. et al* [paper]()

## [Models](#content)


### [Lifelong Learning Models](#content) to be sorted

1. **ELLA: An Efficient Lifelong Learning Algorithm** PMLR 2013. [paper](http://proceedings.mlr.press/v28/ruvolo13.pdf)

1. **Selective Experience Replay for Lifelong Learning** arXiv 2018. [paper](https://arxiv.org/pdf/1802.10269.pdf)
)

### [Regularization Approach](#content)

1. **Learning without Forgetting**  IEEE Transactions on Pattern Analysis and Machine Intelligence 2018.

&emsp;&emsp;&emsp;*Li, Z. & Hoiem, D* [paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8107520)

1. **Less-forgetting learning in deep neural networks.** AAAI 2018.

&emsp;&emsp;&emsp;*Jung, H. et al.* [paper](https://arxiv.org/pdf/1607.00122)

1. **CNN features off-the-shelf: An astounding baseline for recognition** CVPR 2014. 

&emsp;&emsp;&emsp;*Razavian, A. S, et al.* [paper](https://www.cv-foundation.org/openaccess/content_cvpr_workshops_2014/W15/papers/Razavian_CNN_Features_Off-the-Shelf_2014_CVPR_paper.pdf)

1. **Decaf: A deep convolutional activation feature for generic visual recognition.** ICML 2014. 

&emsp;&emsp;&emsp;*Donahue, J. et al.* [paper](http://www.jmlr.org/proceedings/papers/v32/donahue14.pdf) 

1. **Overcoming catastrophic forgetting in neural networks.** PNAS 2017 TNN 1997. 

&emsp;&emsp;&emsp;*Kirkpatrick, J. et al.* [paper](https://www.pnas.org/content/pnas/114/13/3521.full.pdf)

1. **Continual learning through synaptic intelligence.** ICML 2017.

&emsp;&emsp;&emsp;*Zenke, F., Poole, B. & Ganguli, S.* [paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6944509/)

### [Regularization Approach +](#content) 

1. **Continuous learning in single-incremental-task scenarios.** arXiv 2018. 

&emsp;&emsp;&emsp;*Maltoni, D. & Lomonaco, V.* [paper](https://arxiv.org/pdf/2005.04167)

1.  **Life-long learning based on dynamic combination model.** Applied Soft Computing 2017. 

&emsp;&emsp;&emsp;*Ren, B. et al.* [paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6944509/)

1. **Ensemble learning in fixed expansion layer networks for mitigating catastrophic forgetting** IEEE Transactions on Neural Networks and Learning Systems 2013. 

&emsp;&emsp;&emsp;*Coop, R., Mishtal, A. & Arel, I.* [paper](http://web.eecs.utk.edu/~ielhanan/Papers/TNNLS_Coop_2013.pdf)

1. **Pathnet: Evolution channels gradient descent in super neural networks** arXiv 2017. 

&emsp;&emsp;&emsp;*Fernando, C. st al.* [paper](https://arxiv.org/pdf/1701.08734)


### [Dynamic Architecture](#content)

### [Complementary System and Replay](#content)

### [Analysis]

1. **Learning multiple layers of features from tiny images** Masterâ€™s thesis, University
of Toronto 2009. *Krizhevsky, A.* [paper]()
1. **Measuring catastrophic forgetting in neural networks** AAAI 2018. *Kemker, R.* [paper]()

## [Datasets](#content)

1. **[MNIST](#content)** [dataset](http://yann.lecun.com/exdb/mnist/)

1. **[CUB-200](#content)** [dataset](http://www.vision.caltech.edu/visipedia/CUB-200.html)

1. **[AudioSet](#content)** [dataset](https://research.google.com/audioset/)

1. **[Core50](#content)** [dataset](https://vlomonaco.github.io/core50//)

1. **[iCIFAR-100](#content)** [dataset]


## [Applications](#content)  